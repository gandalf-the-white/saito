#+title: Readme
* Introduction
Just to prove that we can use practice *LLM* with *LISP*. 
The agent is composed of different stacks:
+ Client (run-weather-agent)
+ Ollama Client
+ Ollama / LLM
+ Tool Manager
+ Tool Manager

* Explication
** Client (run-weather-agent)
+ Receives the user's question.
+ Builds the initial message with the system prompt.
+ Launches the orchestration loop.
** Ollama Client
+ Encodes messages in JSON.
+ Sends the request to Ollama via HTTP POST (/api/chat).
+ Receives a JSON response.
** Ollama / LLM
+ Produces either:
  + a normal response,
  + a native tool_call,
  + or a fake JSON tool call in the content field.
** Tool Manager
 + Groups all saved tools.
 + Finds the right tool to call, either:
 + via tool_calls,
 + via manual JSON found in content.
** Tools
Each tool is an independent module with:
+ a JSON schema
+ a Lisp handler
+ an entry in the *tools* list
** Example
+ get_time
+ get_weather
** External APIs
The tools can call:
+ Open-Meteo (geocoding + weather)
+ The system time, etc.
** Call Flow (Summary)
1. The user sends a question to run-weather-agent.
2. The agent adds the system prompt and messages.
3. The messages are sent to Ollama.
4. The model responds:
   a. either with a tool_call
   b. or with manual JSON
   c. or with a classic response.
5. If a tool is detected:
   1. The Tool Manager calls the tool handler.
   2. The handler returns a structured response.
6. The final result is returned to the user.

* Graphic
** Agent Flow Diagram 
Just a flow diagram according to a simple agent
#+begin_src plantuml :file ./images/diagram.svg :results silent :exports none
@startuml WeatherAgentSequence

actor Utilisateur
participant "Agent LISP\n(saito)" as agent
participant "Ollama\n(llama3.2)" as ollama
participant "Open-Meteo\n(Geocoding)" as geocoding
participant "Open-Meteo\n(Forecast)" as forecast

Utilisateur -> agent: "Peux-tu me donner\la météo actuelle à Paris ?"
activate agent

agent -> ollama: call-ollama(messages)
activate ollama
ollama --> agent: réponse avec tool_call\n("get_weather", {"city": "Paris"})
deactivate ollama

agent -> agent: handle-tool-call\n(exécute get_weather)
activate agent
agent -> geocoding: geocode-city("Paris")
activate geocoding
geocoding --> agent: (lat, lon, name, country, admin1, timezone)
deactivate geocoding

agent -> forecast: http-request(forecast?lat=...&lon=...)
activate forecast
forecast --> agent: {current_weather: {...}}
deactivate forecast

agent -> agent: formatte la réponse météo
agent -> ollama: envoie le résultat du tool\n(role: "tool", content: "Météo pour Paris...")
activate ollama
ollama --> agent: réponse finale\n("Météo pour Paris : 15.5°C, vent 10 km/h...")
deactivate ollama

agent --> Utilisateur: "Météo pour Paris : 15.5°C,\nvent 10 km/h (direction 180°),\ncode météo 3."
deactivate agent
        
@enduml
#+end_src

[[file:./images/diagram.svg]]

** Functional Diagram
#+begin_src plantuml :file ./images/function.svg :results silent :exports none
@startuml
!option handwritten true

rectangle "Agent" as Agent {
  rectangle "Tool-Using\nAgent" as ToolAgent #LightGreen
  rectangle "Tools" as Tools #LightYellow

  ' Flèches internes
  ToolAgent -down[#DarkGreen]-> Tools : user request
}

 ' --- Blocs externes ---
rectangle "OLLAMA" as OLLAMA #LightBlue
rectangle "External APIs" as ExtLeft #Pink
rectangle "External APIs" as ExtRight #Pink

rectangle "Tool\nConfiguration" as ToolConf
rectangle "Direct Call" as DirectCall

 ' --- Positionnement approximatif ---
Agent -[hidden]right- OLLAMA
Agent -[hidden]right- ToolConf
Agent -[hidden]right- DirectCall
OLLAMA -[hidden]down- ToolConf
ToolConf -[hidden]down- DirectCall
DirectCall -[hidden]down- ExtRight
Tools -[hidden]down- ExtLeft
ExtLeft -[hidden]right- ExtRight

' --- Flots entre éléments ---
ToolAgent -right[#DarkGreen]-> OLLAMA : tool call
OLLAMA -left[#DarkBlue]-> ToolAgent : tool call
ToolConf ..right-> Tools
DirectCall ..right-> Tools

Tools -down[#Orange]-> ExtLeft : direct call
ExtRight -left-> ExtLeft : HTTP request

@enduml
#+end_src

[[./images/function.svg]]

** Client Flow Diagram
#+begin_src plantuml :file ./images/client-diagram.svg :results silent :exports none
@startuml
!option handwritten true

actor User
participant "Lisp client\n(run-mcp-ollama-session)" as Client
participant "MCP server\n(*mcp-url*)" as MCP
participant "Ollama\n(*ollama-url* /api/chat)" as Ollama

== Initialisation ==
User -> Client : user-input
activate Client

' Récupération des tools MCP
Client -> MCP : JSON-RPC \"tools/list\"\n(mcp-tools-list → mcp-http-request → http-post-json)
activate MCP
MCP --> Client : result.tools
deactivate MCP

Client -> Client : mcp-tools->ollama-tools(tools)\n→ ollama-tools
Client -> Client : construire messages\n(system + user)

== Boucle principale ==
loop Conversation
  ' Appel du modèle Ollama
  Client -> Ollama : POST /api/chat\n(call-ollama-with-tools → http-post-json)
  activate Ollama
  Ollama --> Client : message + content + tool_calls?
  deactivate Ollama

  alt tool_calls vide
    Client --> User : content (réponse finale)
    deactivate Client
    break
  else tool_calls présent
    ' Exécution de chaque tool_call
    loop pour chaque tool_call
      Client -> Client : parse-arguments-maybe(arguments)

      Client -> MCP : JSON-RPC \"tools/call\"\n(mcp-call-tool-text → mcp-http-request → http-post-json)
      activate MCP
      MCP --> Client : result.content[0].text
      deactivate MCP

      Client -> Client : push message 'tool'\n(role=tool, tool_name, content)\n→ maj de messages
    end
  end
end

@enduml
#+end_src

[[./images/client-diagram.svg]]
** Server MCP Flow Diagram
Just an overview without the /hunchentoot/ server, imagine 
#+begin_src plantuml :file ./images/server-diagram.svg :results silent :exports none
@startuml
!option handwritten true

actor "MCP Host\n(éditeur / IDE)" as Host
participant "run-mcp-server\n(Lisp MCP tools server)" as Server
participant "Open-Meteo\nAPIs" as OpenMeteo
participant "Ollama\n(*ollama-url*)" as Ollama

loop lecture lignes STDIN
  Host -> Server : ligne JSON-RPC\ninitialize | tools/list | tools/call
  activate Server

  Server -> Server : handle-json-rpc-line(line)\nparse → method,id,params

  alt method == "initialize"
    Server -> Server : handle-initialize(id,params)
    Server --> Host : JSON-RPC result\n(protocolVersion, capabilities, serverInfo)

  else method == "tools/list"
    Server -> Server : handle-tools-list(id,params)\nmap mcp-tool->json sur *mcp-tools*
    Server --> Host : JSON-RPC result\n(liste des tools)

  else method == "tools/call"
    Server -> Server : handle-tools-call(id,params)\nfind-mcp-tool(name)

    alt tool inconnu
      Server --> Host : error -32602\n"Unknown tool"

    else tool connu
      Server -> Server : funcall mcp-tool-handler(tool,arguments)

      alt tool == "get_time"
        Server -> Server : current-time-string(timezone)\n→ texte horodaté

      else tool == "get_weather"
        Server -> Server : geocode-city(city)
        Server -> OpenMeteo : HTTP GET /v1/search\nname=city,...
        OpenMeteo --> Server : JSON lat,lon,timezone,...
        Server -> OpenMeteo : HTTP GET /v1/forecast\nlat,lon,current_weather=1,...
        OpenMeteo --> Server : JSON current_weather
        Server -> Server : get-weather-from-api → résumé météo texte

      else tool == "ollama_chat"
        Server -> Ollama : POST /api/chat\nmodel=*ollama-model*, messages=[user prompt]
        Ollama --> Server : JSON { message.content }
        Server -> Server : call-ollama-chat → texte
      end

      Server --> Host : JSON-RPC result\ncontent[0].text, isError=false
    end

  else méthode inconnue
    Server --> Host : error -32601\n"Unknown method"
  end

  deactivate Server
end

@enduml
#+end_src

[[./images/server-diagram.svg]]

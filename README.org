#+title: Readme
* Introduction
** Simple Agent
Just to prove that we can use practice *LLM* with *LISP*. 
The agent is composed of different stacks:
+ Client (run-weather-agent)
+ Ollama Client
+ Ollama / LLM
+ Tool Manager
+ Tool Manager
** MCP Server
The same objective with 2 parts
+ A server
+ A client
* Explication
** Client (run-weather-agent)
+ Receives the user's question.
+ Builds the initial message with the system prompt.
+ Launches the orchestration loop.
** Ollama Client
+ Encodes messages in JSON.
+ Sends the request to Ollama via HTTP POST (/api/chat).
+ Receives a JSON response.
** Ollama / LLM
+ Produces either:
  + a normal response,
  + a native tool_call,
  + or a fake JSON tool call in the content field.
** Tool Manager
 + Groups all saved tools.
 + Finds the right tool to call, either:
 + via tool_calls,
 + via manual JSON found in content.
** Tools
Each tool is an independent module with:
+ a JSON schema
+ a Lisp handler
+ an entry in the *tools* list
** Example
+ get_time
+ get_weather
** External APIs
The tools can call:
+ Open-Meteo (geocoding + weather)
+ The system time, etc.
** Call Flow (Summary)
1. The user sends a question to run-weather-agent.
2. The agent adds the system prompt and messages.
3. The messages are sent to Ollama.
4. The model responds:
   a. either with a tool_call
   b. or with manual JSON
   c. or with a classic response.
5. If a tool is detected:
   1. The Tool Manager calls the tool handler.
   2. The handler returns a structured response.
6. The final result is returned to the user.

* Graphic
** Agent Flow Diagram 
Just a flow diagram according to a simple agent
#+begin_src plantuml :file ./images/diagram.svg :results silent :exports none
@startuml WeatherAgentSequence
!option handwritten true

actor "MCP Host\n(IDE / éditeur)" as Host
participant "run-mcp-server\n(boucle JSON-RPC)" as Server
participant "Open-Meteo\nAPI" as Meteo
participant "Ollama\n*ollama-url*" as Ollama

== Boucle principale ==
loop jusqu'à EOF sur STDIN
  Host -> Server : ligne JSON-RPC (JSON texte)
  activate Server

  Server -> Server : handle-json-rpc-line(line)\nparse (jonathan:parse)

  alt method == "initialize"
    Server -> Server : handle-initialize(id, params)\nserverInfo + capabilities
    Server -> Host : write-json-response(result)
  
  else method == "tools/list"
    Server -> Server : handle-tools-list(id, params)\n(map mcp-tool->json *mcp-tools*)
    Server -> Host : write-json-response(result)

  else method == "tools/call"
    Server -> Server : handle-tools-call(id, params)\n(find-mcp-tool name)

    alt tool inconnu
      Server -> Server : make-error-response(-32602, "Unknown tool")
      Server -> Host : write-json-response(error)

    else tool trouvé
      alt name == "get_time"
        Server -> Server : tool-get-time(args)\n→ current-time-string(timezone)

      else name == "get_weather"
        Server -> Server : tool-get-weather(args)\n(city, lat, lon)
        Server -> Server : geocode-city(city) (si lat/lon manquants)
        Server -> Meteo : http-get /v1/search\n(geocode-city)
        Meteo --> Server : body (coordonnées)
        Server -> Meteo : http-get /v1/forecast\n(get-weather-from-api)
        Meteo --> Server : body (current_weather)
        Server -> Server : get-weather-from-api\n→ texte météo

      else name == "ollama_chat"
        Server -> Server : tool-ollama-chat(args)\n(extrait prompt)
        Server -> Ollama : http-post-json *ollama-url*\n(call-ollama-chat)
        Ollama --> Server : réponse modèle (message.content)
      end

      Server -> Host : write-json-response(result)
    end

  else méthode inconnue
    Server -> Server : make-error-response(-32601,"Unknown method")
    Server -> Host : write-json-response(error)
  end

  deactivate Server
end        
@enduml
#+end_src

[[file:./images/diagram.svg]]

** Functional Diagram
#+begin_src plantuml :file ./images/function.svg :results silent :exports none
@startuml
!option handwritten true

rectangle "Agent" as Agent {
  rectangle "Tool-Using\nAgent" as ToolAgent #LightGreen
  rectangle "Tools" as Tools #LightYellow

  ' Flèches internes
  ToolAgent -down[#DarkGreen]-> Tools : user request
}

 ' --- Blocs externes ---
rectangle "OLLAMA" as OLLAMA #LightBlue
rectangle "External APIs" as ExtLeft #Pink
rectangle "External APIs" as ExtRight #Pink

rectangle "Tool\nConfiguration" as ToolConf
rectangle "Direct Call" as DirectCall

 ' --- Positionnement approximatif ---
Agent -[hidden]right- OLLAMA
Agent -[hidden]right- ToolConf
Agent -[hidden]right- DirectCall
OLLAMA -[hidden]down- ToolConf
ToolConf -[hidden]down- DirectCall
DirectCall -[hidden]down- ExtRight
Tools -[hidden]down- ExtLeft
ExtLeft -[hidden]right- ExtRight

' --- Flots entre éléments ---
ToolAgent -right[#DarkGreen]-> OLLAMA : tool call
OLLAMA -left[#DarkBlue]-> ToolAgent : tool call
ToolConf ..right-> Tools
DirectCall ..right-> Tools

Tools -down[#Orange]-> ExtLeft : direct call
ExtRight -left-> ExtLeft : HTTP request

@enduml
#+end_src

[[./images/function.svg]]

** Client Flow Diagram
#+begin_src plantuml :file ./images/client-diagram.svg :results silent :exports none
@startuml
!option handwritten true

actor User
participant "Lisp client\n(run-mcp-ollama-session)" as Client
participant "MCP server\n(*mcp-url*)" as MCP
participant "Ollama\n(*ollama-url* /api/chat)" as Ollama

== Initialisation ==
User -> Client : user-input
activate Client

' Récupération des tools MCP
Client -> MCP : JSON-RPC \"tools/list\"\n(mcp-tools-list → mcp-http-request → http-post-json)
activate MCP
MCP --> Client : result.tools
deactivate MCP

Client -> Client : mcp-tools->ollama-tools(tools)\n→ ollama-tools
Client -> Client : construire messages\n(system + user)

== Boucle principale ==
loop Conversation
  ' Appel du modèle Ollama
  Client -> Ollama : POST /api/chat\n(call-ollama-with-tools → http-post-json)
  activate Ollama
  Ollama --> Client : message + content + tool_calls?
  deactivate Ollama

  alt tool_calls vide
    Client --> User : content (réponse finale)
    deactivate Client
    break
  else tool_calls présent
    ' Exécution de chaque tool_call
    loop pour chaque tool_call
      Client -> Client : parse-arguments-maybe(arguments)

      Client -> MCP : JSON-RPC \"tools/call\"\n(mcp-call-tool-text → mcp-http-request → http-post-json)
      activate MCP
      MCP --> Client : result.content[0].text
      deactivate MCP

      Client -> Client : push message 'tool'\n(role=tool, tool_name, content)\n→ maj de messages
    end
  end
end

@enduml
#+end_src

[[./images/client-diagram.svg]]
** Server MCP Flow Diagram
Just an overview without the /hunchentoot/ server, imagine 
#+begin_src plantuml :file ./images/server-diagram.svg :results silent :exports none
@startuml
!option handwritten true

actor "MCP Host\n(éditeur / IDE)" as Host
participant "run-mcp-server\n(Lisp MCP tools server)" as Server
participant "Open-Meteo\nAPIs" as OpenMeteo
participant "Ollama\n(*ollama-url*)" as Ollama

loop lecture lignes STDIN
  Host -> Server : ligne JSON-RPC\ninitialize | tools/list | tools/call
  activate Server

  Server -> Server : handle-json-rpc-line(line)\nparse → method,id,params

  alt method == "initialize"
    Server -> Server : handle-initialize(id,params)
    Server --> Host : JSON-RPC result\n(protocolVersion, capabilities, serverInfo)

  else method == "tools/list"
    Server -> Server : handle-tools-list(id,params)\nmap mcp-tool->json sur *mcp-tools*
    Server --> Host : JSON-RPC result\n(liste des tools)

  else method == "tools/call"
    Server -> Server : handle-tools-call(id,params)\nfind-mcp-tool(name)

    alt tool inconnu
      Server --> Host : error -32602\n"Unknown tool"

    else tool connu
      Server -> Server : funcall mcp-tool-handler(tool,arguments)

      alt tool == "get_time"
        Server -> Server : current-time-string(timezone)\n→ texte horodaté

      else tool == "get_weather"
        Server -> Server : geocode-city(city)
        Server -> OpenMeteo : HTTP GET /v1/search\nname=city,...
        OpenMeteo --> Server : JSON lat,lon,timezone,...
        Server -> OpenMeteo : HTTP GET /v1/forecast\nlat,lon,current_weather=1,...
        OpenMeteo --> Server : JSON current_weather
        Server -> Server : get-weather-from-api → résumé météo texte

      else tool == "ollama_chat"
        Server -> Ollama : POST /api/chat\nmodel=*ollama-model*, messages=[user prompt]
        Ollama --> Server : JSON { message.content }
        Server -> Server : call-ollama-chat → texte
      end

      Server --> Host : JSON-RPC result\ncontent[0].text, isError=false
    end

  else méthode inconnue
    Server --> Host : error -32601\n"Unknown method"
  end

  deactivate Server
end

@enduml
#+end_src

[[./images/server-diagram.svg]]

* Context
** Questions
+ Can you create me a server

** Agent
+ To help you create a server, I need a bit more information:
  + What type of server you want? (VM, instance EC2)
  + Any specification for the server (instance type, operating system)
  + Do you want any networking setup like VPC, subnet, or security groups
  + Please provide these details so I can generate the appropriate Terraform configuration for you.

** Instruction
+ You are a Terraform AI agent that helps users generate valid efficient and secure terraform configuration in a form of .tf files.
  
** Rules
Always follow these rules unless user asks for a specific change
+ You can only provide terraform configuration for AWS cloud provider.
+ You only know how to create t3.micro instances unless user asks for something different 
+ Automatically attach an externa IP address to the instance
+ Create a security group that allow SSH access from anywhere
  + if user question in unclear ask for clarification before running any tools
  + always be helpful  and friendly
  + if you don't know how to answer the question do not make things up. tell the user "Sorry", I don't know how to answer that and ask them to clarify the question further.
  + do not delete the original deployment until the user explicitly confirms that the rollout is ready to take over production.

** Response
+ always format your response as markdown.
+ your response will include a summary of actions you took and an explanation of the result
+ if you created any artifacts such as files or resources you will include those in your response as well
